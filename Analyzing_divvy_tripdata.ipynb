{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "516622ea",
   "metadata": {},
   "source": [
    "# Trip data files stored in a folder are combined into a single dataframe "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38569392",
   "metadata": {},
   "source": [
    "**Important Note-**\n",
    "\n",
    "-All the csv files of trip data are split into two folders because of huge size to process together. \n",
    "\n",
    "-This spliting is done on the basis of file names. All the csv file names like 'Divvy_Trips_2015_07.csv' are stored in one      folder and the remaining csv files having names like '202004-divvy-tripdata.csv' are stored in another folder. \n",
    "\n",
    "-This is done for convenience of handling the data.\n",
    "\n",
    "Folder 1 = All csv files having names like '202004-divvy-tripdata.csv'\n",
    "\n",
    "Folder 2 = All csv files having names like 'Divvy_Trips_2015_07.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2cd2d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "trips_data_file= 'insert the folder path containing trip data files of type- 202004-divvy-tripdata.csv '\n",
    "df_trips=[]\n",
    "for f in os.listdir(trips_data_file):\n",
    "    if f.endswith('.csv'):\n",
    "        df_trips.append(pd.read_csv(os.path.join(trips_data_file,f)))\n",
    "len(df_trips)\n",
    "dftrips=pd.concat(df_trips, axis=0)\n",
    "dftrips.to_csv('store it in desired folder path')\n",
    "print(dftrips)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ceedd",
   "metadata": {},
   "source": [
    "# Analyzing Trip data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca42a40",
   "metadata": {},
   "source": [
    "## Reading '.csv' file and converting into pandas dataframe\n",
    "    1.Importing dataset and converting into pandas dataframe\n",
    "    2.Checking the data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd84a27e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the dataframe from pandas\n",
    "filepath='insert your filepath of stored csv file given from the above cell '\n",
    "data1=pd.read_csv(filepath,low_memory=False,\n",
    "                  dtype={'rideable_type':'category','started_at':'string','ended_at':'string',\n",
    "                         'start_station_name':'string',\n",
    "                         'end_station_name':'string',\n",
    "                         'member_casual':'category'})\n",
    "display(data1)\n",
    "display(data1.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f08b4a",
   "metadata": {},
   "source": [
    "## Data type manipulation and removal of unwanted columns\n",
    "       1.Dataset contains data type which needs to be converted\n",
    "       2.Removal of some columns \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92e20ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#changing the data type for better memory allocation\n",
    "data1=data1.astype({'rideable_type':'category','started_at':'string',\n",
    "                    'ended_at':'string','start_station_name':'string',\n",
    "                    'end_station_name':'string','member_casual':'category'})\n",
    "#removal of any unwanted columns\n",
    "data1=data1.drop(columns=['Unnamed: 0'])\n",
    "#checking for correct data types\n",
    "data1.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df1bc3e8",
   "metadata": {},
   "source": [
    "## Variables information and data cleaning\n",
    "    1.removal of any duplicates and checking the count of na values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f20f29d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#getting to know variables present in the dataset\n",
    "data1.info()\n",
    "#finding any duplicates and remove them\n",
    "data1=data1.drop_duplicates()\n",
    "#getting to know the na values accross columns\n",
    "data1.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b26b50ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding the na values in the dataframe across columns \n",
    "data1.isna().sum()\n",
    "#total na values in dataframe\n",
    "display(data1.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3026055a",
   "metadata": {},
   "source": [
    "## Removal of all Na values\n",
    "    1. Total na values are approximately 3 million records which are removed \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccb17f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf=data1.dropna()\n",
    "display(cleandf.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7f3f37b",
   "metadata": {},
   "source": [
    "## converting time column's dtype into relevant datetime dtype and then formatting  \n",
    "    1.converting to 'datetime' dtype allows better handling of time columns into specific time format\n",
    "    2.It was seen that the cleaned dataframe had time columns with two separate time formats \n",
    "    3.So inorder to solve this issue, the cleaned dataframe was divided into 2 subset of dataframes with separate time format as there is some inconsistency in time format  \n",
    "    4.One of the subset with columns-'started_at' and 'ended_at' has different time format which needs to be corrected to '%Y-%m-%d %H:%M:%S'\n",
    "    5.This can be done by first converting dtype of those time columns into 'datetime64[ns]' and then formatting into '%Y-%m-%d %H:%M:%S'\n",
    "    6.The second subset contains time columns with correct time format and only requires converion of dtype into 'datetime64[ns]' dtype \n",
    "    7.later two subset dataframes can be concatenated into one single dataframe with consistent formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a2e494",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting a subset of dataframe with('ended_at'&'started_at')columns to convert into datetime \n",
    "newdf1=cleandf[(cleandf['started_at'].str.contains('/')) & (cleandf['ended_at'].str.contains('/'))]\n",
    "\n",
    "#converting the ('started_at'&'ended_at')columns into datetime dtype\n",
    "newdf1[['started_at','ended_at']]=pd.to_datetime(newdf1[['started_at','ended_at']].stack(),\n",
    "        infer_datetime_format=True,format='%Y-%m-%d %H:%M:%S').unstack()\n",
    "\n",
    "#adjusting the('started_at'&'ended_at')columns with datetime format    \n",
    "newdf1[['started_at','ended_at']]=newdf1[['started_at','ended_at']].dt.strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "#displaying the subset dataframe with correct datetime format\n",
    "display(newdf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f0d9cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#selecting a subset of dataframe with('ended_at'&'started_at')column to convert into datetime \n",
    "newdf2=cleandf[(cleandf['started_at'].str.contains('-')) & (cleandf['ended_at'].str.contains('-'))]\n",
    "\n",
    "#converting the ('ended_at'&'ended_at') column into datetime dtype\n",
    "newdf2[['started_at','ended_at']]=pd.to_datetime(newdf2[['started_at','ended_at']].stack(),\n",
    "                           infer_datetime_format=True,format='%Y-%m-%d %H:%M:%S').unstack()\n",
    "\n",
    "#displaying the subset dataframe with correct datetime format\n",
    "display(newdf2)\n",
    "\n",
    "#combining two dataframes with same time formats \n",
    "newdf=pd.concat([newdf1,newdf2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ad1ae2",
   "metadata": {},
   "source": [
    "## Adding new columns to calculate trip duration and trip distance\n",
    "    1.'trip_duration' column is added by subtracting relevant time columns and showing duration in hours\n",
    "    2.'trip_distance' column is added by calculating distance between two geographical coordintes using haversine distance         formula by importing relevant library 'mpu' and units are in kms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "14c6a3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adding column for calculating time duration for the trip\n",
    "newdf['trip_duration']=(newdf.ended_at-newdf.started_at).dt.total_seconds()/3600\n",
    "\n",
    "#sorting the dataframe with time \n",
    "new1=newdf.sort_values(['started_at','ended_at']).reset_index(drop=True)\n",
    "\n",
    "#adding column for calculating distance between two places(haversine dist)\n",
    "new1['trip_distance']=list((map(lambda x,y :mpu.haversine_distance(x,y),zip(new1['start_lat'],\n",
    "                    new1['start_lng']),zip(new1['end_lat'],new1['end_lng']))))\n",
    "\n",
    "#rounding off distance to two places\n",
    "new1['trip_distance']=new1['trip_distance'].round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d022e7",
   "metadata": {},
   "source": [
    "## Completing data cleaning,manipulation,pre-processing steps to store data into new file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "bc24ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "new1.to_csv('filepath of csv file to store in local with names like 2020-2023.csv ') #contains the trip data from 2020-2023"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
